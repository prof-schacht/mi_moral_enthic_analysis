{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love you!\n",
      "\n",
      "Detoxify Scores:\n",
      "toxicity: 0.001\n",
      "severe_toxicity: 0.000\n",
      "obscene: 0.000\n",
      "threat: 0.000\n",
      "insult: 0.000\n",
      "identity_attack: 0.000\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "def get_detoxify_scores(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get toxicity scores using Detoxify model\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing scores for different attributes\n",
    "        \n",
    "    Library Page: https://pypi.org/project/detoxify/\n",
    "        \n",
    "    \"\"\"\n",
    "    model = Detoxify('original')\n",
    "    scores = model.predict(text)\n",
    "    return scores\n",
    "\n",
    "# Example usage:\n",
    "test_text = \"I love you!\"\n",
    "scores = get_detoxify_scores(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(\"\\nDetoxify Scores:\")\n",
    "for attribute, score in scores.items():\n",
    "    print(f\"{attribute}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': 0.00078165694,\n",
       " 'severe_toxicity': 0.00012962069,\n",
       " 'obscene': 0.00020162243,\n",
       " 'threat': 0.00014777461,\n",
       " 'insult': 0.00017766617,\n",
       " 'identity_attack': 0.00015764897}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity': [0.0028345452155917883]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "toxicity = evaluate.load(\"ybelkada/toxicity\", \"DaNLP/da-electra-hatespeech-detection\", module_type=\"measurement\")\n",
    "toxicity_score = toxicity.compute(predictions=[\"I love you!\"])\n",
    "toxicity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset_name = \"Salesforce/wikitext\"\n",
    "split = \"wikitext-2-v1\"\n",
    "dataset = datasets.load_dataset(dataset_name, name=split, split=\"test\")\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Wesley <unk> ( <unk> , Wes <unk> ) — guitar ( 1994 – 2003 ) \\n'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][500]\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
